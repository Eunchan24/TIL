{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "epochs = 5\n",
    "no_cuda = False\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(5*5*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5*5*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths = glob('/Users/eunchankim/Desktop/dataset/cifar/train/*.png')\n",
    "test_paths = glob('/Users/eunchankim/Desktop/dataset/cifar/test/*.png')\n",
    "\n",
    "len(train_paths), len(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/eunchankim/Desktop/dataset/cifar/train/32270_deer.png'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = train_paths[0]\n",
    "# path\n",
    "\n",
    "# lbl_name = os.path.basename(path).split('_')[-1].replace('.png','')\n",
    "# lbl_name\n",
    "\n",
    "def get_label(path):\n",
    "    lbl_name = os.path.basename(path).split('_')[-1].replace('.png','')\n",
    "    label = np.argmax(classes == lbl_name)\n",
    "    return label\n",
    "\n",
    "# img_pil = Image.open(path)\n",
    "# image = np.array(img_pil)\n",
    "# image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_paths, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data_paths[idx]\n",
    "        # read image\n",
    "        image = Image.open(path)\n",
    "        \n",
    "        # get label\n",
    "        label = get_label(path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs ={'num_workers':1, 'pin_menory':True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    Dataset(train_paths,\n",
    "           transforms.Compose([\n",
    "               transforms.RandomHorizontalFlip(),\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize(\n",
    "               mean=[0.406],\n",
    "               std=[0.225]\n",
    "               )])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    "    )\n",
    "\n",
    "# test\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    Dataset(test_paths,\n",
    "           transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize(\n",
    "               mean=[0.406],\n",
    "               std=[0.225]\n",
    "               )])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.311073\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.279047\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.279692\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.248241\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.241749\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.188693\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.081832\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.074457\n",
      "\n",
      "Test set: Average loss: 2.0560, Accuracy: 2695/10000 (27%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.159764\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.963291\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.089917\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.862567\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.991901\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.977339\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.976192\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.935786\n",
      "\n",
      "Test set: Average loss: 1.9117, Accuracy: 3265/10000 (33%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.015244\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.805102\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.787864\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.966308\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.852215\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.877121\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.751932\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.912617\n",
      "\n",
      "Test set: Average loss: 1.8023, Accuracy: 3627/10000 (36%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.698985\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.643975\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.792689\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.892252\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.653174\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.756753\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.545072\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.807070\n",
      "\n",
      "Test set: Average loss: 1.6793, Accuracy: 4016/10000 (40%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.744592\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.664816\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.742628\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.543916\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.792037\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.535089\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.685758\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.691870\n",
      "\n",
      "Test set: Average loss: 1.5832, Accuracy: 4315/10000 (43%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    # Test mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
